{"ast":null,"code":"import _construct from \"/Users/louiskrause/neuefische/RespiratoryApp/st_audiorec/frontend/node_modules/@babel/runtime/helpers/esm/construct\";\nimport _toConsumableArray from \"/Users/louiskrause/neuefische/RespiratoryApp/st_audiorec/frontend/node_modules/@babel/runtime/helpers/esm/toConsumableArray\";\n// Licensed to the Apache Software Foundation (ASF) under one\n// or more contributor license agreements.  See the NOTICE file\n// distributed with this work for additional information\n// regarding copyright ownership.  The ASF licenses this file\n// to you under the Apache License, Version 2.0 (the\n// \"License\"); you may not use this file except in compliance\n// with the License.  You may obtain a copy of the License at\n//\n//   http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing,\n// software distributed under the License is distributed on an\n// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n// KIND, either express or implied.  See the License for the\n// specific language governing permissions and limitations\n// under the License.\nimport { Data } from '../data';\nimport { Schema } from '../schema';\nimport { Chunked } from '../vector/chunked';\nimport { RecordBatch } from '../recordbatch';\nvar noopBuf = new Uint8Array(0);\n\nvar nullBufs = function nullBufs(bitmapLength) {\n  return [noopBuf, noopBuf, new Uint8Array(bitmapLength), noopBuf];\n};\n/** @ignore */\n\n\nexport function ensureSameLengthData(schema, chunks) {\n  var batchLength = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : chunks.reduce(function (l, c) {\n    return Math.max(l, c.length);\n  }, 0);\n  var data;\n  var field;\n  var i = -1,\n      n = chunks.length;\n\n  var fields = _toConsumableArray(schema.fields);\n\n  var batchData = [];\n  var bitmapLength = (batchLength + 63 & ~63) >> 3;\n\n  while (++i < n) {\n    if ((data = chunks[i]) && data.length === batchLength) {\n      batchData[i] = data;\n    } else {\n      (field = fields[i]).nullable || (fields[i] = fields[i].clone({\n        nullable: true\n      }));\n      batchData[i] = data ? data._changeLengthAndBackfillNullBitmap(batchLength) : Data.new(field.type, 0, batchLength, batchLength, nullBufs(bitmapLength));\n    }\n  }\n\n  return [new Schema(fields), batchLength, batchData];\n}\n/** @ignore */\n\nexport function distributeColumnsIntoRecordBatches(columns) {\n  return distributeVectorsIntoRecordBatches(new Schema(columns.map(function (_ref) {\n    var field = _ref.field;\n    return field;\n  })), columns);\n}\n/** @ignore */\n\nexport function distributeVectorsIntoRecordBatches(schema, vecs) {\n  return uniformlyDistributeChunksAcrossRecordBatches(schema, vecs.map(function (v) {\n    return v instanceof Chunked ? v.chunks.map(function (c) {\n      return c.data;\n    }) : [v.data];\n  }));\n}\n/** @ignore */\n\nfunction uniformlyDistributeChunksAcrossRecordBatches(schema, columns) {\n  var fields = _toConsumableArray(schema.fields);\n\n  var batchArgs = [];\n  var memo = {\n    numBatches: columns.reduce(function (n, c) {\n      return Math.max(n, c.length);\n    }, 0)\n  };\n  var numBatches = 0,\n      batchLength = 0;\n  var i = -1,\n      numColumns = columns.length;\n  var child,\n      childData = [];\n\n  while (memo.numBatches-- > 0) {\n    for (batchLength = Number.POSITIVE_INFINITY, i = -1; ++i < numColumns;) {\n      childData[i] = child = columns[i].shift();\n      batchLength = Math.min(batchLength, child ? child.length : batchLength);\n    }\n\n    if (isFinite(batchLength)) {\n      childData = distributeChildData(fields, batchLength, childData, columns, memo);\n\n      if (batchLength > 0) {\n        batchArgs[numBatches++] = [batchLength, childData.slice()];\n      }\n    }\n  }\n\n  return [schema = new Schema(fields, schema.metadata), batchArgs.map(function (xs) {\n    return _construct(RecordBatch, [schema].concat(_toConsumableArray(xs)));\n  })];\n}\n/** @ignore */\n\n\nfunction distributeChildData(fields, batchLength, childData, columns, memo) {\n  var data;\n  var field;\n  var length = 0,\n      i = -1,\n      n = columns.length;\n  var bitmapLength = (batchLength + 63 & ~63) >> 3;\n\n  while (++i < n) {\n    if ((data = childData[i]) && (length = data.length) >= batchLength) {\n      if (length === batchLength) {\n        childData[i] = data;\n      } else {\n        childData[i] = data.slice(0, batchLength);\n        data = data.slice(batchLength, length - batchLength);\n        memo.numBatches = Math.max(memo.numBatches, columns[i].unshift(data));\n      }\n    } else {\n      (field = fields[i]).nullable || (fields[i] = field.clone({\n        nullable: true\n      }));\n      childData[i] = data ? data._changeLengthAndBackfillNullBitmap(batchLength) : Data.new(field.type, 0, batchLength, batchLength, nullBufs(bitmapLength));\n    }\n  }\n\n  return childData;\n}","map":{"version":3,"mappings":";;AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAKA,SAASA,IAAT,QAA8B,SAA9B;AACA,SAASC,MAAT,QAA8B,WAA9B;AACA,SAASC,OAAT,QAAwB,mBAAxB;AACA,SAASC,WAAT,QAA4B,gBAA5B;AAEA,IAAMC,OAAO,GAAG,IAAIC,UAAJ,CAAe,CAAf,CAAhB;;AACA,IAAMC,QAAQ,GAAG,SAAXA,QAAW,CAACC,YAAD;AAAA,SAAoC,CACjDH,OADiD,EACxCA,OADwC,EAC/B,IAAIC,UAAJ,CAAeE,YAAf,CAD+B,EACDH,OADC,CAApC;AAAA,CAAjB;AAIA;;;AACA,OAAM,SAAUI,oBAAV,CACFC,MADE,EAEFC,MAFE,EAG6D;AAAA,MAA/DC,WAA+D,uEAAjDD,MAAM,CAACE,MAAP,CAAc,UAACC,CAAD,EAAIC,CAAJ;AAAA,WAAUC,IAAI,CAACC,GAAL,CAASH,CAAT,EAAYC,CAAC,CAACG,MAAd,CAAV;AAAA,GAAd,EAA+C,CAA/C,CAAiD;AAE/D,MAAIC,IAAJ;AACA,MAAIC,KAAJ;AACA,MAAIC,CAAC,GAAG,CAAC,CAAT;AAAA,MAAYC,CAAC,GAAGX,MAAM,CAACO,MAAvB;;AACA,MAAMK,MAAM,sBAAOb,MAAM,CAACa,MAAd,CAAZ;;AACA,MAAMC,SAAS,GAAG,EAAlB;AACA,MAAMhB,YAAY,GAAG,CAAEI,WAAW,GAAG,EAAf,GAAqB,CAAC,EAAvB,KAA8B,CAAnD;;AACA,SAAO,EAAES,CAAF,GAAMC,CAAb,EAAgB;AACZ,QAAI,CAACH,IAAI,GAAGR,MAAM,CAACU,CAAD,CAAd,KAAsBF,IAAI,CAACD,MAAL,KAAgBN,WAA1C,EAAuD;AACnDY,eAAS,CAACH,CAAD,CAAT,GAAeF,IAAf;AACH,KAFD,MAEO;AACH,OAACC,KAAK,GAAGG,MAAM,CAACF,CAAD,CAAf,EAAoBI,QAApB,KAAiCF,MAAM,CAACF,CAAD,CAAN,GAAYE,MAAM,CAACF,CAAD,CAAN,CAAUK,KAAV,CAAgB;AAAED,gBAAQ,EAAE;AAAZ,OAAhB,CAA7C;AACAD,eAAS,CAACH,CAAD,CAAT,GAAeF,IAAI,GAAGA,IAAI,CAACQ,kCAAL,CAAwCf,WAAxC,CAAH,GACbX,IAAI,CAAC2B,GAAL,CAASR,KAAK,CAACS,IAAf,EAAqB,CAArB,EAAwBjB,WAAxB,EAAqCA,WAArC,EAAkDL,QAAQ,CAACC,YAAD,CAA1D,CADN;AAEH;AACJ;;AACD,SAAO,CAAC,IAAIN,MAAJ,CAAcqB,MAAd,CAAD,EAAwBX,WAAxB,EAAqCY,SAArC,CAAP;AACH;AAED;;AACA,OAAM,SAAUM,kCAAV,CAA0FC,OAA1F,EAAuH;AACzH,SAAOC,kCAAkC,CAAI,IAAI9B,MAAJ,CAAc6B,OAAO,CAACE,GAAR,CAAY;AAAA,QAAGb,KAAH,QAAGA,KAAH;AAAA,WAAeA,KAAf;AAAA,GAAZ,CAAd,CAAJ,EAAsDW,OAAtD,CAAzC;AACH;AAED;;AACA,OAAM,SAAUC,kCAAV,CAA0FtB,MAA1F,EAA6GwB,IAA7G,EAA+J;AACjK,SAAOC,4CAA4C,CAAIzB,MAAJ,EAAYwB,IAAI,CAACD,GAAL,CAAS,UAACG,CAAD;AAAA,WAAOA,CAAC,YAAYjC,OAAb,GAAuBiC,CAAC,CAACzB,MAAF,CAASsB,GAAT,CAAa,UAAClB,CAAD;AAAA,aAAOA,CAAC,CAACI,IAAT;AAAA,KAAb,CAAvB,GAAqD,CAACiB,CAAC,CAACjB,IAAH,CAA5D;AAAA,GAAT,CAAZ,CAAnD;AACH;AAED;;AACA,SAASgB,4CAAT,CAAmGzB,MAAnG,EAAsHqB,OAAtH,EAAmJ;AAE/I,MAAMR,MAAM,sBAAOb,MAAM,CAACa,MAAd,CAAZ;;AACA,MAAMc,SAAS,GAAG,EAAlB;AACA,MAAMC,IAAI,GAAG;AAAEC,cAAU,EAAER,OAAO,CAAClB,MAAR,CAAe,UAACS,CAAD,EAAIP,CAAJ;AAAA,aAAUC,IAAI,CAACC,GAAL,CAASK,CAAT,EAAYP,CAAC,CAACG,MAAd,CAAV;AAAA,KAAf,EAAgD,CAAhD;AAAd,GAAb;AAEA,MAAIqB,UAAU,GAAG,CAAjB;AAAA,MAAoB3B,WAAW,GAAG,CAAlC;AACA,MAAIS,CAAC,GAAW,CAAC,CAAjB;AAAA,MAAoBmB,UAAU,GAAGT,OAAO,CAACb,MAAzC;AACA,MAAIuB,KAAJ;AAAA,MAA6BC,SAAS,GAAuB,EAA7D;;AAEA,SAAOJ,IAAI,CAACC,UAAL,KAAoB,CAA3B,EAA8B;AAE1B,SAAK3B,WAAW,GAAG+B,MAAM,CAACC,iBAArB,EAAwCvB,CAAC,GAAG,CAAC,CAAlD,EAAqD,EAAEA,CAAF,GAAMmB,UAA3D,GAAwE;AACpEE,eAAS,CAACrB,CAAD,CAAT,GAAeoB,KAAK,GAAGV,OAAO,CAACV,CAAD,CAAP,CAAWwB,KAAX,EAAvB;AACAjC,iBAAW,GAAGI,IAAI,CAAC8B,GAAL,CAASlC,WAAT,EAAsB6B,KAAK,GAAGA,KAAK,CAACvB,MAAT,GAAkBN,WAA7C,CAAd;AACH;;AAED,QAAImC,QAAQ,CAACnC,WAAD,CAAZ,EAA2B;AACvB8B,eAAS,GAAGM,mBAAmB,CAACzB,MAAD,EAASX,WAAT,EAAsB8B,SAAtB,EAAiCX,OAAjC,EAA0CO,IAA1C,CAA/B;;AACA,UAAI1B,WAAW,GAAG,CAAlB,EAAqB;AACjByB,iBAAS,CAACE,UAAU,EAAX,CAAT,GAA0B,CAAC3B,WAAD,EAAc8B,SAAS,CAACO,KAAV,EAAd,CAA1B;AACH;AACJ;AACJ;;AACD,SAAO,CACHvC,MAAM,GAAG,IAAIR,MAAJ,CAAcqB,MAAd,EAAsBb,MAAM,CAACwC,QAA7B,CADN,EAEHb,SAAS,CAACJ,GAAV,CAAc,UAACkB,EAAD;AAAA,sBAAY/C,WAAZ,GAAwBM,MAAxB,4BAAmCyC,EAAnC;AAAA,GAAd,CAFG,CAAP;AAIH;AAED;;;AACA,SAASH,mBAAT,CAA0EzB,MAA1E,EAAuGX,WAAvG,EAA4H8B,SAA5H,EAA2JX,OAA3J,EAA0LO,IAA1L,EAAsN;AAClN,MAAInB,IAAJ;AACA,MAAIC,KAAJ;AACA,MAAIF,MAAM,GAAG,CAAb;AAAA,MAAgBG,CAAC,GAAG,CAAC,CAArB;AAAA,MAAwBC,CAAC,GAAGS,OAAO,CAACb,MAApC;AACA,MAAMV,YAAY,GAAG,CAAEI,WAAW,GAAG,EAAf,GAAqB,CAAC,EAAvB,KAA8B,CAAnD;;AACA,SAAO,EAAES,CAAF,GAAMC,CAAb,EAAgB;AACZ,QAAI,CAACH,IAAI,GAAGuB,SAAS,CAACrB,CAAD,CAAjB,KAA0B,CAACH,MAAM,GAAGC,IAAI,CAACD,MAAf,KAA0BN,WAAxD,EAAsE;AAClE,UAAIM,MAAM,KAAKN,WAAf,EAA4B;AACxB8B,iBAAS,CAACrB,CAAD,CAAT,GAAeF,IAAf;AACH,OAFD,MAEO;AACHuB,iBAAS,CAACrB,CAAD,CAAT,GAAeF,IAAI,CAAC8B,KAAL,CAAW,CAAX,EAAcrC,WAAd,CAAf;AACAO,YAAI,GAAGA,IAAI,CAAC8B,KAAL,CAAWrC,WAAX,EAAwBM,MAAM,GAAGN,WAAjC,CAAP;AACA0B,YAAI,CAACC,UAAL,GAAkBvB,IAAI,CAACC,GAAL,CAASqB,IAAI,CAACC,UAAd,EAA0BR,OAAO,CAACV,CAAD,CAAP,CAAW+B,OAAX,CAAmBjC,IAAnB,CAA1B,CAAlB;AACH;AACJ,KARD,MAQO;AACH,OAACC,KAAK,GAAGG,MAAM,CAACF,CAAD,CAAf,EAAoBI,QAApB,KAAiCF,MAAM,CAACF,CAAD,CAAN,GAAYD,KAAK,CAACM,KAAN,CAAY;AAAED,gBAAQ,EAAE;AAAZ,OAAZ,CAA7C;AACAiB,eAAS,CAACrB,CAAD,CAAT,GAAeF,IAAI,GAAGA,IAAI,CAACQ,kCAAL,CAAwCf,WAAxC,CAAH,GACbX,IAAI,CAAC2B,GAAL,CAASR,KAAK,CAACS,IAAf,EAAqB,CAArB,EAAwBjB,WAAxB,EAAqCA,WAArC,EAAkDL,QAAQ,CAACC,YAAD,CAA1D,CADN;AAEH;AACJ;;AACD,SAAOkC,SAAP;AACH","names":["Data","Schema","Chunked","RecordBatch","noopBuf","Uint8Array","nullBufs","bitmapLength","ensureSameLengthData","schema","chunks","batchLength","reduce","l","c","Math","max","length","data","field","i","n","fields","batchData","nullable","clone","_changeLengthAndBackfillNullBitmap","new","type","distributeColumnsIntoRecordBatches","columns","distributeVectorsIntoRecordBatches","map","vecs","uniformlyDistributeChunksAcrossRecordBatches","v","batchArgs","memo","numBatches","numColumns","child","childData","Number","POSITIVE_INFINITY","shift","min","isFinite","distributeChildData","slice","metadata","xs","unshift"],"sources":["util/recordbatch.ts"],"sourcesContent":["// Licensed to the Apache Software Foundation (ASF) under one\n// or more contributor license agreements.  See the NOTICE file\n// distributed with this work for additional information\n// regarding copyright ownership.  The ASF licenses this file\n// to you under the Apache License, Version 2.0 (the\n// \"License\"); you may not use this file except in compliance\n// with the License.  You may obtain a copy of the License at\n//\n//   http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing,\n// software distributed under the License is distributed on an\n// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n// KIND, either express or implied.  See the License for the\n// specific language governing permissions and limitations\n// under the License.\n\nimport { Column } from '../column';\nimport { Vector } from '../vector';\nimport { DataType } from '../type';\nimport { Data, Buffers } from '../data';\nimport { Schema, Field } from '../schema';\nimport { Chunked } from '../vector/chunked';\nimport { RecordBatch } from '../recordbatch';\n\nconst noopBuf = new Uint8Array(0);\nconst nullBufs = (bitmapLength: number) => <unknown> [\n    noopBuf, noopBuf, new Uint8Array(bitmapLength), noopBuf\n] as Buffers<any>;\n\n/** @ignore */\nexport function ensureSameLengthData<T extends { [key: string]: DataType } = any>(\n    schema: Schema<T>,\n    chunks: Data<T[keyof T]>[],\n    batchLength = chunks.reduce((l, c) => Math.max(l, c.length), 0)\n) {\n    let data: Data<T[keyof T]>;\n    let field: Field<T[keyof T]>;\n    let i = -1, n = chunks.length;\n    const fields = [...schema.fields];\n    const batchData = [] as Data<T[keyof T]>[];\n    const bitmapLength = ((batchLength + 63) & ~63) >> 3;\n    while (++i < n) {\n        if ((data = chunks[i]) && data.length === batchLength) {\n            batchData[i] = data;\n        } else {\n            (field = fields[i]).nullable || (fields[i] = fields[i].clone({ nullable: true }) as Field<T[keyof T]>);\n            batchData[i] = data ? data._changeLengthAndBackfillNullBitmap(batchLength)\n                : Data.new(field.type, 0, batchLength, batchLength, nullBufs(bitmapLength)) as Data<T[keyof T]>;\n        }\n    }\n    return [new Schema<T>(fields), batchLength, batchData] as [Schema<T>, number, Data<T[keyof T]>[]];\n}\n\n/** @ignore */\nexport function distributeColumnsIntoRecordBatches<T extends { [key: string]: DataType } = any>(columns: Column<T[keyof T]>[]): [Schema<T>, RecordBatch<T>[]] {\n    return distributeVectorsIntoRecordBatches<T>(new Schema<T>(columns.map(({ field }) => field)), columns);\n}\n\n/** @ignore */\nexport function distributeVectorsIntoRecordBatches<T extends { [key: string]: DataType } = any>(schema: Schema<T>, vecs: (Vector<T[keyof T]> | Chunked<T[keyof T]>)[]): [Schema<T>, RecordBatch<T>[]] {\n    return uniformlyDistributeChunksAcrossRecordBatches<T>(schema, vecs.map((v) => v instanceof Chunked ? v.chunks.map((c) => c.data) : [v.data]));\n}\n\n/** @ignore */\nfunction uniformlyDistributeChunksAcrossRecordBatches<T extends { [key: string]: DataType } = any>(schema: Schema<T>, columns: Data<T[keyof T]>[][]): [Schema<T>, RecordBatch<T>[]] {\n\n    const fields = [...schema.fields];\n    const batchArgs = [] as [number, Data<T[keyof T]>[]][];\n    const memo = { numBatches: columns.reduce((n, c) => Math.max(n, c.length), 0) };\n\n    let numBatches = 0, batchLength = 0;\n    let i: number = -1, numColumns = columns.length;\n    let child: Data<T[keyof T]>, childData: Data<T[keyof T]>[] = [];\n\n    while (memo.numBatches-- > 0) {\n\n        for (batchLength = Number.POSITIVE_INFINITY, i = -1; ++i < numColumns;) {\n            childData[i] = child = columns[i].shift()!;\n            batchLength = Math.min(batchLength, child ? child.length : batchLength);\n        }\n\n        if (isFinite(batchLength)) {\n            childData = distributeChildData(fields, batchLength, childData, columns, memo);\n            if (batchLength > 0) {\n                batchArgs[numBatches++] = [batchLength, childData.slice()];\n            }\n        }\n    }\n    return [\n        schema = new Schema<T>(fields, schema.metadata),\n        batchArgs.map((xs) => new RecordBatch(schema, ...xs))\n    ];\n}\n\n/** @ignore */\nfunction distributeChildData<T extends { [key: string]: DataType } = any>(fields: Field<T[keyof T]>[], batchLength: number, childData: Data<T[keyof T]>[], columns: Data<T[keyof T]>[][], memo: { numBatches: number }) {\n    let data: Data<T[keyof T]>;\n    let field: Field<T[keyof T]>;\n    let length = 0, i = -1, n = columns.length;\n    const bitmapLength = ((batchLength + 63) & ~63) >> 3;\n    while (++i < n) {\n        if ((data = childData[i]) && ((length = data.length) >= batchLength)) {\n            if (length === batchLength) {\n                childData[i] = data;\n            } else {\n                childData[i] = data.slice(0, batchLength);\n                data = data.slice(batchLength, length - batchLength);\n                memo.numBatches = Math.max(memo.numBatches, columns[i].unshift(data));\n            }\n        } else {\n            (field = fields[i]).nullable || (fields[i] = field.clone({ nullable: true }) as Field<T[keyof T]>);\n            childData[i] = data ? data._changeLengthAndBackfillNullBitmap(batchLength)\n                : Data.new(field.type, 0, batchLength, batchLength, nullBufs(bitmapLength)) as Data<T[keyof T]>;\n        }\n    }\n    return childData;\n}\n"]},"metadata":{},"sourceType":"module"}